{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rv7EcwVV60XI"
   },
   "source": [
    "\n",
    "# NMT with Transformer&BPE\n",
    "\n",
    "\n",
    "**To do:**\n",
    "\n",
    "- BPE (youtokentome lib)\n",
    "- nn.Transformer\n",
    "- Translater en <--> ru\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cvbhn1jq60XL"
   },
   "outputs": [],
   "source": [
    "! pip install youtokentome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3MUZpJtx60XM"
   },
   "source": [
    "We will use \n",
    "https://github.com/VKCOM/YouTokenToMe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "cjQaaAQh60XN"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MAN-MADE\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "import unicodedata\n",
    "\n",
    "import youtokentome as yttm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v5k2bMex60XN"
   },
   "source": [
    "## Data\n",
    "\n",
    "Same en+ru corpus. But this time, as we will represent the data not in the form of words, but in the form of sub-word parts, using Byte Pair Encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "07qm82Cc60XP"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "830f449ca8984933809391ed855cdd5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/336666 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW alphabet 174 symbols: \n",
      " !\"$%&'()+,-./0123456789:;?@ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz «°º»ãçéêîïóöúǘЁАБВГДЕЖЗИЙКЛМНОПРСТУФХЦЧШЩЬЭЮЯабвгдежзийклмнопрстуфхцчшщъыьэюяёׁ​–—―‘’… ‽₂€№\n",
      "After preprocessing 62 symbols:   !,.?abcdefghijklmnopqrstuvwxyzабвгдежзиклмнопрстуфхцчшщъыьэюя\n",
      "There are 336666 pairs\n",
      "['let him do it .', 'пусть он это сделает !']\n"
     ]
    }
   ],
   "source": [
    "# Prepare data and look at it\n",
    "# In addition to the dictionary, we are also interested in a set of characters\n",
    "raw_alphabet = set()\n",
    "alphabet = set()\n",
    "def normalize(s):\n",
    "    return \"\".join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def preprocess(s):\n",
    "    raw_alphabet.update(s)\n",
    "    s = normalize(s.lower().strip())\n",
    "    s = re.sub(r\"[^a-zа-я?.,!]+\", \" \", s)\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    alphabet.update(s)\n",
    "    return s\n",
    "\n",
    "pairs = []\n",
    "with open('eng-rus.txt', 'rb') as fin:\n",
    "    for line in tqdm(fin.readlines()):\n",
    "        pair = [preprocess(_) for _ in line.decode('utf-8').split('\\t')]\n",
    "        pairs.append(pair)\n",
    "\n",
    "print(\"RAW alphabet {} symbols:\".format(len(raw_alphabet)), \n",
    "      \"\".join(sorted(raw_alphabet)))\n",
    "print(\"After preprocessing {} symbols: \".format(len(alphabet)), \n",
    "      \"\".join(sorted(alphabet)))\n",
    "print(\"There are {} pairs\".format(len(pairs)))\n",
    "print(pairs[10101])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jlkbYFv460XP"
   },
   "source": [
    "## BPE training\n",
    "\n",
    "BPE allows us to train a dictionary of arbitrary sizes.\n",
    "\n",
    "For example, we can make a common dictionary for English and Russian.\n",
    "To do this, you need to write all available texts into one file and train BPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "OjK1ig2l60XP"
   },
   "outputs": [],
   "source": [
    "lines = []\n",
    "for p in pairs:\n",
    "    lines += p\n",
    "lines = list(set(lines))\n",
    "with open(\"./all.txt\", \"wb\") as fout:\n",
    "    for line in lines:\n",
    "        fout.write((line + \"\\n\").encode('utf-8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "qHJDqwVe60XQ"
   },
   "outputs": [],
   "source": [
    "VOCAB = 5000\n",
    "bpe = yttm.BPE.train(data=\"./all.txt\", vocab_size=VOCAB, model=\"enru.bpe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EAvzIcw060XQ"
   },
   "source": [
    "\n",
    "## NB: is it possible to train BPE on the whole dataset\n",
    "\n",
    "In many tasks, the question about calculating statistics on the entire dataset may arise:\n",
    "\n",
    "<mark> if _something_ is an important feature, should this _something_ be calculates only on the train, or can we take the entire dataset with validation?</mark>\n",
    "\n",
    "- is it possible to calculate averages over all available data for the task of time series forecasting\n",
    "- is it possible to calculate word2vec on the whole dataset\n",
    "- etc.\n",
    "\n",
    "There is no simple answer, in this case BPE is not literally a model, but changing statistics can affect the composition of the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "0C5MG63i60XR"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[84, 2498, 162, 763, 695, 2455, 66]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe.encode(lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "_RP1hoHT60XR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[84, 2498, 162, 763, 695, 2455, 66], [82, 349, 4161, 45, 567, 89, 4073, 66], [273, 152, 153, 770, 3029, 128, 3132, 83], [84, 2079, 4584, 213, 115, 2072, 3202, 66], [260, 301, 2485, 3655, 2159, 83], [120, 245, 2994, 41, 460, 66], [84, 3978, 433, 161, 319, 857, 1993, 1472, 66], [166, 107, 897, 369, 350, 2754, 64, 83], [89, 802, 221, 8, 132, 495, 2245, 389, 32, 4554, 66], [169, 80, 326, 673, 752, 66]]\n",
      "[['▁не', '▁уро', 'ни', '▁этот', '▁ста', 'кан', '▁.'], ['▁том', '▁был', '▁выше', ',', '▁чем', '▁я', '▁ожидал', '▁.'], ['▁как', '▁бы', '▁вы', '▁пере', 'вели', '▁это', '▁предложение', '▁?'], ['▁не', '▁отве', 'чаи', '▁ни', '▁на', '▁какие', '▁вопросы', '▁.'], ['▁what', '▁are', '▁those', '▁flowers', '▁called', '▁?'], ['▁у', '▁тома', '▁кри', 'з', 'ис', '▁.'], ['▁не', '▁хлеб', 'ом', '▁е', 'ди', 'ным', '▁жив', '▁человек', '▁.'], ['▁do', '▁you', '▁still', '▁like', '▁j', 'az', 'z', '▁?'], ['▁я', '▁только', '▁при', 'т', 'во', 'ря', 'лась', '▁сп', 'я', 'щеи', '▁.'], ['▁that', '▁s', '▁your', '▁lo', 'ss', '▁.']]\n"
     ]
    }
   ],
   "source": [
    "print(bpe.encode(lines[:10], output_type=yttm.OutputType.ID))\n",
    "print(bpe.encode(lines[:10], output_type=yttm.OutputType.SUBWORD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SkmMWmiQ60XR"
   },
   "source": [
    "## BPE Dropout\n",
    " \n",
    " \n",
    "(Article: [BPE-Dropout: Simple and Effective Subword Regularization](https://arxiv.org/abs/1910.13267))\n",
    "\n",
    "In very large BPE dictionaries (5k tokens for two languages is a small dictionary), there is a problem: some tokens are in the dictionary, but are not found in the train data.\n",
    "\n",
    "They can occur in real data, due to natural processes or typos. To deal with this phenomenon and simply as a regularization, you can use BPE-dropout: random re-partitioning of a string into tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "UOg275eK60XR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['▁не', '▁уро', 'ни', '▁этот', '▁ста', 'кан', '▁.']]\n",
      "[['▁', 'не', '▁уро', 'ни', '▁этот', '▁ста', 'кан', '▁.']]\n",
      "[['▁', 'н', 'е', '▁у', 'ро', 'ни', '▁', 'э', 'то', 'т', '▁', 'с', 'т', 'а', 'к', 'а', 'н', '▁.']]\n"
     ]
    }
   ],
   "source": [
    "print(bpe.encode(lines[:1], dropout_prob=0.0, output_type=yttm.OutputType.SUBWORD))\n",
    "print(bpe.encode(lines[:1], dropout_prob=0.2, output_type=yttm.OutputType.SUBWORD))\n",
    "print(bpe.encode(lines[:1], dropout_prob=0.5, output_type=yttm.OutputType.SUBWORD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "HGJWVXJi60XS"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MAN-MADE\\anaconda3\\lib\\site-packages\\seaborn\\distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded0 = [len(_) for _ in bpe.encode(lines, dropout_prob=0.0)]\n",
    "encoded1 = [len(_) for _ in bpe.encode(lines, dropout_prob=0.1)]\n",
    "encoded2 = [len(_) for _ in bpe.encode(lines, dropout_prob=0.5)]\n",
    "\n",
    "sns.distplot(encoded0, kde=False, label=\"no do\")\n",
    "sns.distplot(encoded1, kde=False, label=\"do 0.1\")\n",
    "sns.distplot(encoded2, kde=False, label=\"do 0.5\")\n",
    "plt.legend()\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "e8F_12Uz60XS"
   },
   "outputs": [],
   "source": [
    "# it is proposed to limit the maximum string length to 100 tokens, and to use BPE_DO=0.1 for training\n",
    "MAX_LENGTH = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68JtQmby60XS"
   },
   "source": [
    "##dataset\n",
    "\n",
    "The dataset this time returns a dictionary with en and ru strings, without transformations.\n",
    "\n",
    "collate_fn is not required for us, and we will describe the conversion to BPE inside the model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "wqFVPIqB60XS"
   },
   "outputs": [],
   "source": [
    "class Pairset:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        en, ru = self.data[item]\n",
    "        return dict(en=en, ru=ru)\n",
    "\n",
    "train_pairs, val_pairs = train_test_split(pairs, test_size=0.3)\n",
    "\n",
    "trainset = Pairset(train_pairs)\n",
    "valset = Pairset(val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "8b_d08k660XS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'en': ['please don t talk .', 'tom s answer surprised mary .', 'what exactly are you suggesting ?', 'i ll carry this .', 'don t you want to kiss me ?', 'i wish we could spend more time together .', 'you are gorgeous .', 'would you like a dog ?', 'i don t think tom was surprised .', 'who makes breakfast for tom ?', 'tom gave everything he had .', 'i don t speak your language .', 'thanks for supporting me .', 'i must confer with my colleagues on the matter .', 'you re not that old .', 'tom told me where to buy what i needed .'], 'ru': ['не говори, пожалуиста .', 'ответ тома удивил мэри .', 'что именно ты предлагаешь ?', 'я понесу это .', 'не хотите меня поцеловать ?', 'я бы хотел, чтобы мы могли проводить больше времени вместе .', 'ты великолепна .', 'ты бы хотел собаку ?', 'не думаю, что том удивился .', 'кто готовит завтрак для тома ?', 'том отдал все, что у него было .', 'я не говорю на вашем языке .', 'спасибо за поддержку .', 'я должен посовещаться с коллегами по этому вопросу .', 'вы не такая старая .', 'том сказал мне, где я могу купить то, что мне нужно .']}\n"
     ]
    }
   ],
   "source": [
    "trainloader = DataLoader(trainset, batch_size=16, shuffle=True)\n",
    "it = iter(trainloader)\n",
    "print(next(it))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7xOwGmL560XS"
   },
   "source": [
    "## Train Loop\n",
    "It is assumed that the model has two methods:\n",
    "\n",
    "```\n",
    "model.compute_all(batch) -> Dict\n",
    "model.check_translations(batch) -> None\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "v9UndQS560XT"
   },
   "outputs": [],
   "source": [
    "def train_model(model, opt, trainloader, valloader, epochs=1):\n",
    "    step = 0\n",
    "    logs = defaultdict(list)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for batch in tqdm(trainloader):\n",
    "            details = model.compute_all(batch)\n",
    "            loss = details[\"loss\"]\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            step += 1\n",
    "            [logs[k].append(v) for k, v in details[\"metrics\"].items()]\n",
    "            \n",
    "        model.eval()\n",
    "        tmp = defaultdict(list)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(valloader):\n",
    "                details = model.compute_all(batch)\n",
    "                for k, v in details[\"metrics\"].items():\n",
    "                    tmp[k].append(v)\n",
    "            tmp = {k: np.mean(v) for k, v in tmp.items()}\n",
    "            [logs[f\"val_{k}\"].append(v) for k, v in tmp.items()]\n",
    "            logs[\"step\"].append(step)\n",
    "            model.check_translations(batch)\n",
    "        \n",
    "        for key in [\"loss\"]:\n",
    "            plt.figure()\n",
    "            plt.title(key)\n",
    "            plt.plot(logs[key], label=\"train\", c='b', zorder=1)\n",
    "            plt.scatter(logs[\"step\"], logs[f\"val_{key}\"], label=\"val\", c='r', zorder=10)\n",
    "            plt.legend()\n",
    "            plt.grid()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hv44Q1TG60XT"
   },
   "source": [
    "## `nn.Transformer`\n",
    "\n",
    "The official documentation for (nn.Transformer)[https://pytorch.org/docs/master/generated/torch.nn.Transformer.html#transformer] is rather sparse.\n",
    "\n",
    "But the important points are:\n",
    "\n",
    "0. You need to prepare input and output data yourself: you will need to write positional and token embeddings, as well as an output FC layer\n",
    "\n",
    "1. nn.Transformer.forward takes on running the encoder and applying the decoder correctly.\n",
    "\n",
    "2. The order of the axes is the same as when using RNN models (for compatibility in seq2seq tasks): `[seq_len, batch_size, dimension]`.\n",
    "\n",
    "3. Be sure to set `src_key_padding_mask` and `tgt_key_padding_mask` to mask inaccessible tokens (in particular paddings).\n",
    "\n",
    "\n",
    "\n",
    "It is proposed to have two special tokens: for translation into Russian and into English, with numbers `bpe.vocab_size()` and `bpe.vocab_size() + 1`.\n",
    "These tokens may not be generated using the output layer, but they may be at the input.\n",
    "\n",
    "\n",
    "\n",
    "It is proposed to write the following functions:\n",
    "\n",
    "```\n",
    "model.encode(list_of_strings) # a function that converts a string into a sequence of BPE token numbers, adds special tokens and padd to MAX_LENGTH\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "model.check_translations(batch) # a function that will make and display the translation for a batch with examples\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "model.compute_all(batch) # function for training, will run the batch, calculate the loss and return a dictionary with metrics and loss\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "xUng4hk060XT"
   },
   "outputs": [],
   "source": [
    "class VeryT(nn.Module):\n",
    "    def __init__(self, bpe, bpe_dropout=0.1, hidden_size=256):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bpe = bpe\n",
    "        self.bpe_dropout = bpe_dropout\n",
    "        self.embeddings = nn.Embedding(bpe.vocab_size() + 2, hidden_size)\n",
    "        self.positional_embeddings = nn.Embedding(MAX_LENGTH, hidden_size)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=hidden_size, \n",
    "            nhead=8, \n",
    "            num_encoder_layers=3, \n",
    "            num_decoder_layers=3, dim_feedforward=512)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, bpe.vocab_size()),\n",
    "            nn.LogSoftmax(dim=-1),\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def encode(self, lst, pre=None, post=None, seq_len=None, dropout=0.0):\n",
    "        lst = [self.bpe.encode(entry, dropout_prob=dropout) for entry in lst]\n",
    "        ## add tokens and paddings\n",
    "        #<your code>\n",
    "        print(lst)\n",
    "        return lst\n",
    "        \n",
    "    def check_translations(self, batch):\n",
    "        en, ru = [batch[key] for key in [\"en\", \"ru\"]]\n",
    "        src = self.encode(en)\n",
    "        dst = self.encode(ru)\n",
    "        #<your code if needed>\n",
    "        src = torch.LongTensor(src)\n",
    "        dst = torch.LongTensor(dst)\n",
    "        with torch.no_grad():\n",
    "            # generate ouput autoregressively\n",
    "            for i in range(10):  # MAX_LEN - 1\n",
    "                print('kek')\n",
    "                #<your code>\n",
    "            dst = dst.cpu().numpy()\n",
    "            dst = [line.tolist() for line in dst]\n",
    "            dst = self.bpe.decode(dst)\n",
    "            dst = [line.replace(\"<PAD>\", \"\") for line in dst]\n",
    "        for line in zip(en, ru, dst):\n",
    "            print(\"\\t\".join(line))\n",
    "    \n",
    "    def compute_all(self, batch):\n",
    "        en, ru = [batch[key] for key in [\"en\", \"ru\"]]\n",
    "        #<formulate task>\n",
    "        #src = self.encode(en, ...)\n",
    "        #dst = self.encode(ru, ... )\n",
    "        \n",
    "        src = torch.LongTensor(src)\n",
    "        dst = torch.LongTensor(dst)\n",
    "\n",
    "        output = self.forward(src, dst)\n",
    "        \n",
    "        #<compute loss>\n",
    "        #loss = ...\n",
    "        \n",
    "        return dict(\n",
    "            loss=loss,\n",
    "            metrics=dict(\n",
    "                loss=loss.item(),\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, src, dst):\n",
    "        # let's a little hack:\n",
    "        device = next(self.parameters()).device\n",
    "        src = src.to(device)\n",
    "        dst = dst.to(device)\n",
    "        \n",
    "        \n",
    "        #<build embeddings for tokens and positional>\n",
    "        \n",
    "        # embedded = embedded_tokens * sqrt(hidden_size) + embedded_positions\n",
    "        \n",
    "        #<reshape properly>\n",
    "        \n",
    "        #<build pad masks>\n",
    "        src_pad_mask = src != 0\n",
    "        dst_pad_mask = dst != 0\n",
    "        \n",
    "        output = self.transformer(src_embedded, dst_embedded, \n",
    "                                  src_key_padding_mask=src_pad_mask, \n",
    "                                  tgt_key_padding_mask=dst_pad_mask)\n",
    "        #<predict next token probs>\n",
    "        #<permute to [bs, vocab, seq_len]> \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<youtokentome.youtokentome.BPE at 0x28cdef22160>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "OjPj8m7p60XU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[251, 156, 77, 244, 103, 66], [198, 199, 3047, 287, 48, 248, 66], [91, 390, 662, 829, 1031, 124, 426, 1656, 2708, 440, 233, 66], [70, 1262, 107, 278, 513, 208, 316, 251, 66], [70, 231, 67, 333, 169, 139, 400, 716, 66], [70, 92, 858, 369, 72, 263, 66], [91, 615, 416, 963, 208, 1523, 66], [70, 231, 67, 237, 105, 1186, 177, 594, 1574, 260, 139, 446, 166, 1325, 66], [107, 467, 72, 1006, 66], [198, 156, 305, 3950, 424, 2449, 582, 2786, 953, 66], [91, 446, 2189, 66], [647, 70, 991, 564, 1518, 340, 4553, 45, 70, 615, 2372, 72, 564, 66], [70, 263, 91, 390, 4850, 72, 445, 66], [107, 202, 3457, 208, 199, 3511, 522, 66], [105, 904, 156, 630, 3207, 66], [91, 244, 3456, 105, 2619, 103, 66]]\n",
      "[[128, 1291, 24, 66], [128, 408, 2872, 495, 1521, 56, 368, 66], [82, 510, 2835, 151, 109, 85, 225, 1130, 2764, 66], [1277, 158, 612, 2795, 811, 183, 74, 1199, 66], [89, 84, 472, 116, 223, 2371, 66], [89, 152, 1210, 2118, 2027, 66], [82, 671, 152, 4439, 255, 3898, 66], [84, 1429, 56, 213, 2266, 280, 3123, 2687, 3869, 45, 116, 223, 601, 624, 4899, 66], [401, 556, 2196, 66], [634, 84, 4917, 1773, 4914, 66], [82, 1410, 421, 66], [616, 152, 89, 2338, 639, 2183, 137, 4102, 45, 89, 152, 671, 1607, 1710, 143, 66], [89, 552, 116, 82, 454, 1935, 3406, 66], [158, 1998, 332, 269, 255, 93, 1051, 217, 835, 66], [2317, 878, 1376, 438, 32, 66], [82, 302, 163, 4890, 3544, 298, 66]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 6 at dim 1 (got 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13556/2086266839.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_translations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13556/1307472584.py\u001b[0m in \u001b[0;36mcheck_translations\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mdst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mru\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;31m#<your code if needed>\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0msrc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[0mdst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: expected sequence of length 6 at dim 1 (got 7)"
     ]
    }
   ],
   "source": [
    "# check dimensions\n",
    "model = VeryT(bpe)\n",
    "with torch.no_grad():\n",
    "    batch = next(it)\n",
    "    model.check_translations(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ejyUpPc60XU"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device=\"cuda:0\"\n",
    "print(device)\n",
    "\n",
    "model = VeryT(bpe)\n",
    "model.to(device)\n",
    "opt = optim.Adam(model.parameters(), lr=3e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jTzXiNkA60XU"
   },
   "outputs": [],
   "source": [
    "trainloader = DataLoader(trainset, batch_size=50, shuffle=True)\n",
    "valloader = DataLoader(valset, batch_size=50, shuffle=False)\n",
    "\n",
    "train_model(model, opt, trainloader, valloader)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
