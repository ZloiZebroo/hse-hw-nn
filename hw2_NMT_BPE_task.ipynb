{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rv7EcwVV60XI"
      },
      "source": [
        "\n",
        "# NMT with Transformer&BPE\n",
        "\n",
        "\n",
        "**To do:**\n",
        "\n",
        "- BPE (youtokentome lib)\n",
        "- nn.Transformer\n",
        "- Translater en <--> ru\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cvbhn1jq60XL"
      },
      "outputs": [],
      "source": [
        "! pip install youtokentome"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MUZpJtx60XM"
      },
      "source": [
        "We will use \n",
        "https://github.com/VKCOM/YouTokenToMe\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjQaaAQh60XN"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "from IPython.display import clear_output\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import re\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "from tqdm.auto import tqdm\n",
        "import unicodedata\n",
        "\n",
        "import youtokentome as yttm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5k2bMex60XN"
      },
      "source": [
        "## Data\n",
        "\n",
        "Same en+ru corpus. But this time, as we will represent the data not in the form of words, but in the form of sub-word parts, using Byte Pair Encoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07qm82Cc60XP"
      },
      "outputs": [],
      "source": [
        "# Prepare data and look at it\n",
        "# In addition to the dictionary, we are also interested in a set of characters\n",
        "raw_alphabet = set()\n",
        "alphabet = set()\n",
        "def normalize(s):\n",
        "    return \"\".join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "\n",
        "def preprocess(s):\n",
        "    raw_alphabet.update(s)\n",
        "    s = normalize(s.lower().strip())\n",
        "    s = re.sub(r\"[^a-zа-я?.,!]+\", \" \", s)\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    alphabet.update(s)\n",
        "    return s\n",
        "\n",
        "pairs = []\n",
        "with open('eng-rus.txt', 'r') as fin:\n",
        "    for line in tqdm(fin.readlines()):\n",
        "        pair = [preprocess(_) for _ in line.split('\\t')]\n",
        "        pairs.append(pair)\n",
        "\n",
        "print(\"RAW alphabet {} symbols:\".format(len(raw_alphabet)), \n",
        "      \"\".join(sorted(raw_alphabet)))\n",
        "print(\"After preprocessing {} symbols: \".format(len(alphabet)), \n",
        "      \"\".join(sorted(alphabet)))\n",
        "print(\"There are {} pairs\".format(len(pairs)))\n",
        "print(pairs[10101])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlkbYFv460XP"
      },
      "source": [
        "## BPE training\n",
        "\n",
        "BPE allows us to train a dictionary of arbitrary sizes.\n",
        "\n",
        "For example, we can make a common dictionary for English and Russian.\n",
        "To do this, you need to write all available texts into one file and train BPE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjK1ig2l60XP"
      },
      "outputs": [],
      "source": [
        "lines = []\n",
        "for p in pairs:\n",
        "    lines += p\n",
        "lines = list(set(lines))\n",
        "with open(\"./all.txt\", \"w\") as fout:\n",
        "    for line in lines:\n",
        "        fout.write(line + \"\\n\")\n",
        "\n",
        "! head all.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHJDqwVe60XQ"
      },
      "outputs": [],
      "source": [
        "VOCAB = 5000\n",
        "bpe = yttm.BPE.train(data=\"./all.txt\", vocab_size=VOCAB, model=\"enru.bpe\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAvzIcw060XQ"
      },
      "source": [
        "\n",
        "## NB: is it possible to train BPE on the whole dataset\n",
        "\n",
        "In many tasks, the question about calculating statistics on the entire dataset may arise:\n",
        "\n",
        "<mark> if _something_ is an important feature, should this _something_ be calculates only on the train, or can we take the entire dataset with validation?</mark>\n",
        "\n",
        "- is it possible to calculate averages over all available data for the task of time series forecasting\n",
        "- is it possible to calculate word2vec on the whole dataset\n",
        "- etc.\n",
        "\n",
        "There is no simple answer, in this case BPE is not literally a model, but changing statistics can affect the composition of the dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0C5MG63i60XR"
      },
      "outputs": [],
      "source": [
        "bpe.encode(lines[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RP1hoHT60XR"
      },
      "outputs": [],
      "source": [
        "print(bpe.encode(lines[:10], output_type=yttm.OutputType.ID))\n",
        "print(bpe.encode(lines[:10], output_type=yttm.OutputType.SUBWORD))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkmMWmiQ60XR"
      },
      "source": [
        "## BPE Dropout\n",
        " \n",
        " \n",
        "(Article: [BPE-Dropout: Simple and Effective Subword Regularization](https://arxiv.org/abs/1910.13267))\n",
        "\n",
        "In very large BPE dictionaries (5k tokens for two languages is a small dictionary), there is a problem: some tokens are in the dictionary, but are not found in the train data.\n",
        "\n",
        "They can occur in real data, due to natural processes or typos. To deal with this phenomenon and simply as a regularization, you can use BPE-dropout: random re-partitioning of a string into tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOg275eK60XR"
      },
      "outputs": [],
      "source": [
        "print(bpe.encode(lines[:1], dropout_prob=0.0, output_type=yttm.OutputType.SUBWORD))\n",
        "print(bpe.encode(lines[:1], dropout_prob=0.2, output_type=yttm.OutputType.SUBWORD))\n",
        "print(bpe.encode(lines[:1], dropout_prob=0.5, output_type=yttm.OutputType.SUBWORD))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGJWVXJi60XS"
      },
      "outputs": [],
      "source": [
        "encoded0 = [len(_) for _ in bpe.encode(lines, dropout_prob=0.0)]\n",
        "encoded1 = [len(_) for _ in bpe.encode(lines, dropout_prob=0.1)]\n",
        "encoded2 = [len(_) for _ in bpe.encode(lines, dropout_prob=0.5)]\n",
        "\n",
        "sns.distplot(encoded0, kde=False, label=\"no do\")\n",
        "sns.distplot(encoded1, kde=False, label=\"do 0.1\")\n",
        "sns.distplot(encoded2, kde=False, label=\"do 0.5\")\n",
        "plt.legend()\n",
        "plt.yscale('log')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8F_12Uz60XS"
      },
      "outputs": [],
      "source": [
        "# it is proposed to limit the maximum string length to 100 tokens, and to use BPE_DO=0.1 for training\n",
        "MAX_LENGTH = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68JtQmby60XS"
      },
      "source": [
        "##dataset\n",
        "\n",
        "The dataset this time returns a dictionary with en and ru strings, without transformations.\n",
        "\n",
        "collate_fn is not required for us, and we will describe the conversion to BPE inside the model class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqFVPIqB60XS"
      },
      "outputs": [],
      "source": [
        "class Pairset:\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, item):\n",
        "        en, ru = self.data[item]\n",
        "        return dict(en=en, ru=ru)\n",
        "\n",
        "train_pairs, val_pairs = train_test_split(pairs, test_size=0.3)\n",
        "\n",
        "trainset = Pairset(train_pairs)\n",
        "valset = Pairset(val_pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8b_d08k660XS"
      },
      "outputs": [],
      "source": [
        "trainloader = DataLoader(trainset, batch_size=16, shuffle=True)\n",
        "it = iter(trainloader)\n",
        "print(next(it))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xOwGmL560XS"
      },
      "source": [
        "## Train Loop\n",
        "It is assumed that the model has two methods:\n",
        "\n",
        "```\n",
        "model.compute_all(batch) -> Dict\n",
        "model.check_translations(batch) -> None\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9UndQS560XT"
      },
      "outputs": [],
      "source": [
        "def train_model(model, opt, trainloader, valloader, epochs=1):\n",
        "    step = 0\n",
        "    logs = defaultdict(list)\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for batch in tqdm(trainloader):\n",
        "            details = model.compute_all(batch)\n",
        "            loss = details[\"loss\"]\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            step += 1\n",
        "            [logs[k].append(v) for k, v in details[\"metrics\"].items()]\n",
        "            \n",
        "        model.eval()\n",
        "        tmp = defaultdict(list)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(valloader):\n",
        "                details = model.compute_all(batch)\n",
        "                for k, v in details[\"metrics\"].items():\n",
        "                    tmp[k].append(v)\n",
        "            tmp = {k: np.mean(v) for k, v in tmp.items()}\n",
        "            [logs[f\"val_{k}\"].append(v) for k, v in tmp.items()]\n",
        "            logs[\"step\"].append(step)\n",
        "            model.check_translations(batch)\n",
        "        \n",
        "        for key in [\"loss\"]:\n",
        "            plt.figure()\n",
        "            plt.title(key)\n",
        "            plt.plot(logs[key], label=\"train\", c='b', zorder=1)\n",
        "            plt.scatter(logs[\"step\"], logs[f\"val_{key}\"], label=\"val\", c='r', zorder=10)\n",
        "            plt.legend()\n",
        "            plt.grid()\n",
        "            plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hv44Q1TG60XT"
      },
      "source": [
        "## `nn.Transformer`\n",
        "\n",
        "The official documentation for (nn.Transformer)[https://pytorch.org/docs/master/generated/torch.nn.Transformer.html#transformer] is rather sparse.\n",
        "\n",
        "But the important points are:\n",
        "\n",
        "0. You need to prepare input and output data yourself: you will need to write positional and token embeddings, as well as an output FC layer\n",
        "\n",
        "1. nn.Transformer.forward takes on running the encoder and applying the decoder correctly.\n",
        "\n",
        "2. The order of the axes is the same as when using RNN models (for compatibility in seq2seq tasks): `[seq_len, batch_size, dimension]`.\n",
        "\n",
        "3. Be sure to set `src_key_padding_mask` and `tgt_key_padding_mask` to mask inaccessible tokens (in particular paddings).\n",
        "\n",
        "\n",
        "\n",
        "It is proposed to have two special tokens: for translation into Russian and into English, with numbers `bpe.vocab_size()` and `bpe.vocab_size() + 1`.\n",
        "These tokens may not be generated using the output layer, but they may be at the input.\n",
        "\n",
        "\n",
        "\n",
        "It is proposed to write the following functions:\n",
        "\n",
        "```\n",
        "model.encode(list_of_strings) # a function that converts a string into a sequence of BPE token numbers, adds special tokens and padd to MAX_LENGTH\n",
        "```\n",
        "\n",
        "\n",
        "```\n",
        "model.check_translations(batch) # a function that will make and display the translation for a batch with examples\n",
        "```\n",
        "\n",
        "\n",
        "```\n",
        "model.compute_all(batch) # function for training, will run the batch, calculate the loss and return a dictionary with metrics and loss\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUng4hk060XT"
      },
      "outputs": [],
      "source": [
        "class VeryT(nn.Module):\n",
        "    def __init__(self, bpe, bpe_dropout=0.1, hidden_size=256):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.bpe = bpe\n",
        "        self.bpe_dropout = bpe_dropout\n",
        "        self.embeddings = nn.Embedding(bpe.vocab_size() + 2, hidden_size)\n",
        "        self.positional_embeddings = nn.Embedding(MAX_LENGTH, hidden_size)\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=hidden_size, \n",
        "            nhead=8, \n",
        "            num_encoder_layers=3, \n",
        "            num_decoder_layers=3, dim_feedforward=512)\n",
        "        \n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(hidden_size, bpe.vocab_size()),\n",
        "            nn.LogSoftmax(dim=-1),\n",
        "        )\n",
        "        \n",
        "        \n",
        "    def encode(self, lst, pre=None, post=None, seq_len=None, dropout=0.0):\n",
        "        lst = [self.bpe.encode(entry, dropout_prob=dropout) for entry in lst]\n",
        "        ## add tokens and paddings\n",
        "        <your code>\n",
        "        return lst\n",
        "        \n",
        "    def check_translations(self, batch):\n",
        "        en, ru = [batch[key] for key in [\"en\", \"ru\"]]\n",
        "        src = self.encode(en, ...)\n",
        "        dst = self.encode(ru, ...)\n",
        "        <your code if needed>\n",
        "        src = torch.LongTensor(src)\n",
        "        dst = torch.LongTensor(dst)\n",
        "        with torch.no_grad():\n",
        "            # generate ouput autoregressively\n",
        "            for i in range(10):  # MAX_LEN - 1\n",
        "                <your code>\n",
        "            dst = dst.cpu().numpy()\n",
        "            dst = [line.tolist() for line in dst]\n",
        "            dst = self.bpe.decode(dst)\n",
        "            dst = [line.replace(\"<PAD>\", \"\") for line in dst]\n",
        "        for line in zip(en, ru, dst):\n",
        "            print(\"\\t\".join(line))\n",
        "    \n",
        "    def compute_all(self, batch):\n",
        "        en, ru = [batch[key] for key in [\"en\", \"ru\"]]\n",
        "        <formulate task>\n",
        "        src = self.encode(en, ...)\n",
        "        dst = self.encode(ru, ... )\n",
        "        \n",
        "        src = torch.LongTensor(src)\n",
        "        dst = torch.LongTensor(dst)\n",
        "\n",
        "        output = self.forward(src, dst)\n",
        "        \n",
        "        <compute loss>\n",
        "        loss = ...\n",
        "        \n",
        "        return dict(\n",
        "            loss=loss,\n",
        "            metrics=dict(\n",
        "                loss=loss.item(),\n",
        "            )\n",
        "        )\n",
        "        \n",
        "        \n",
        "    def forward(self, src, dst):\n",
        "        # let's a little hack:\n",
        "        device = next(self.parameters()).device\n",
        "        src = src.to(device)\n",
        "        dst = dst.to(device)\n",
        "        \n",
        "        \n",
        "        <build embeddings for tokens and positional>\n",
        "        \n",
        "        # embedded = embedded_tokens * sqrt(hidden_size) + embedded_positions\n",
        "        \n",
        "        <reshape properly>\n",
        "        \n",
        "        <build pad masks>\n",
        "        src_pad_mask = src != 0\n",
        "        dst_pad_mask = dst != 0\n",
        "        \n",
        "        output = self.transformer(src_embedded, dst_embedded, \n",
        "                                  src_key_padding_mask=src_pad_mask, \n",
        "                                  tgt_key_padding_mask=dst_pad_mask)\n",
        "        <predict next token probs>\n",
        "        <permute to [bs, vocab, seq_len]> \n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjPj8m7p60XU"
      },
      "outputs": [],
      "source": [
        "# check dimensions\n",
        "model = VeryT(bpe)\n",
        "with torch.no_grad():\n",
        "    batch = next(it)\n",
        "    model.check_translations(batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ejyUpPc60XU"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device=\"cuda:0\"\n",
        "print(device)\n",
        "\n",
        "model = VeryT(bpe)\n",
        "model.to(device)\n",
        "opt = optim.Adam(model.parameters(), lr=3e-4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTzXiNkA60XU"
      },
      "outputs": [],
      "source": [
        "trainloader = DataLoader(trainset, batch_size=50, shuffle=True)\n",
        "valloader = DataLoader(valset, batch_size=50, shuffle=False)\n",
        "\n",
        "train_model(model, opt, trainloader, valloader)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}